# GenAI RAG with MCP - Complete Tutorial & Implementation

A production-ready FastAPI application demonstrating **Retrieval-Augmented Generation (RAG)** using OpenAI embeddings, FAISS vector database, and LLM integration. This project serves as a comprehensive guide to understanding and implementing RAG systems from scratch.

> **Note**: This repository focuses on GenAI fundamentals and RAG implementation. Future MCP (Model Context Protocol) integration planned.

---

## ğŸ¯ What is RAG?

**RAG (Retrieval-Augmented Generation)** combines information retrieval with LLM generation to provide accurate, context-aware answers by:

1. **Searching** through your documents
2. **Finding** relevant information
3. **Injecting** that information into the LLM prompt
4. **Generating** answers based on retrieved context

### Why RAG?

- âœ… **Accuracy**: Answers based on your specific documents
- âœ… **Relevance**: Finds most relevant information automatically
- âœ… **Transparency**: Can cite sources
- âœ… **Up-to-date**: Update documents without retraining LLM
- âœ… **Domain-specific**: Works with any domain knowledge

---

## ğŸš€ Features

### Core RAG Features
- ğŸ“„ **PDF Document Processing**: Extract and process PDF documents
- âœ‚ï¸ **Intelligent Chunking**: Split documents into optimal chunks with overlap
- ğŸ”¢ **Embedding Generation**: Convert text to vectors using OpenAI embeddings
- ğŸ’¾ **Vector Database**: Store and search using FAISS (local, no cloud needed)
- ğŸ” **Semantic Search**: Find relevant information by meaning, not keywords
- ğŸ¤– **LLM Integration**: Generate answers using OpenAI GPT models
- ğŸ”„ **RAG vs Non-RAG Comparison**: Compare answers with and without context

### Production Features
- âœ… **Unified Response Format**: Consistent API responses
- âœ… **Input Validation**: Comprehensive request validation
- âœ… **Error Handling**: Robust error handling with custom exceptions
- âœ… **Timeout Management**: Configurable timeouts for all operations
- âœ… **Logging**: Structured logging throughout
- âœ… **Entity Resolution**: Smart name recognition (e.g., "Anant" â†’ "Anant Kumar Yadav")
- âœ… **Natural Responses**: Conversational, concise answers

---

## ğŸ“‹ Prerequisites

- Python 3.9+
- OpenAI API Key
- pip (Python package manager)

---

## ğŸ› ï¸ Installation

### 1. Clone the Repository

```bash
git clone <your-repo-url>
cd genai-rag-mcp
```

### 2. Install Dependencies

```bash
pip install -r requirement.txt
```

### 3. Set Up Environment Variables

Create a `.env` file in the project root:

```env
# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-3.5-turbo

# Timeout Configuration (in seconds)
REQUEST_TIMEOUT=30
STREAM_TIMEOUT=60

# Message Length Limits
MAX_MESSAGE_LENGTH=10000
MAX_SYSTEM_MESSAGE_LENGTH=5000
```

### 4. Create Data Directory

```bash
mkdir -p data
```

---

## ğŸš€ Quick Start

### 1. Start the Server

```bash
python main.py
```

The API will be available at `http://localhost:8000`

### 2. Ingest a Document

```bash
curl -X POST "http://localhost:8000/rag/ingest" \
  -H "Content-Type: application/json" \
  -d '{
    "pdf_path": "your_document.pdf"
  }'
```

### 3. Query with RAG

```bash
curl -X POST "http://localhost:8000/rag/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is the main topic of the document?",
    "top_k": 3
  }'
```

---

## ğŸ“š How RAG Works - Complete Flow

### Phase 1: Document Processing (One-Time Setup)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Load PDF Document                    â”‚
â”‚     Input: your_document.pdf            â”‚
â”‚     Output: Raw text content            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Split into Chunks                   â”‚
â”‚     - Size: 1000 characters             â”‚
â”‚     - Overlap: 200 characters           â”‚
â”‚     - Preserve sentence boundaries      â”‚
â”‚     Output: List of text chunks         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Generate Embeddings                â”‚
â”‚     - Convert each chunk to vector      â”‚
â”‚     - Using OpenAI text-embedding-3-smallâ”‚
â”‚     - Dimension: 1536 numbers           â”‚
â”‚     Output: Vector embeddings           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Store in Vector Database (FAISS)   â”‚
â”‚     - Store vectors + text + metadata   â”‚
â”‚     - Save to disk for persistence      â”‚
â”‚     Output: Searchable vector index     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 2: Query Processing (Runtime)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. User Query                         â”‚
â”‚     Input: "What is the main topic?"    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. Enhance Query                      â”‚
â”‚     - Entity resolution                 â”‚
â”‚     - Query expansion                  â”‚
â”‚     Output: Enhanced query             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. Generate Query Embedding           â”‚
â”‚     - Convert query to vector           â”‚
â”‚     - Same embedding model              â”‚
â”‚     Output: Query vector               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  8. Search Vector Database             â”‚
â”‚     - Compare query with all vectors    â”‚
â”‚     - Calculate similarity scores       â”‚
â”‚     - Return top-K most similar        â”‚
â”‚     Output: Relevant chunks            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  9. Filter & Rank Context               â”‚
â”‚     - Filter by similarity threshold    â”‚
â”‚     - Rank by relevance                â”‚
â”‚     Output: Top-quality chunks         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  10. Build Prompt with Context          â”‚
â”‚      - Inject retrieved chunks          â”‚
â”‚      - Add instructions                â”‚
â”‚      Output: Enhanced prompt           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  11. Generate Answer                    â”‚
â”‚      - LLM processes prompt            â”‚
â”‚      - Generates context-aware answer  â”‚
â”‚      Output: Final answer              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Architecture

### Project Structure

```
genai-rag-mcp/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config.py                 # Configuration settings
â”‚   â”œâ”€â”€ main.py                   # Application entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                 # Business logic
â”‚   â”‚   â”œâ”€â”€ document_processor.py # PDF loading & chunking
â”‚   â”‚   â”œâ”€â”€ embedding_service.py  # Embedding generation
â”‚   â”‚   â”œâ”€â”€ vector_store.py       # FAISS vector store
â”‚   â”‚   â”œâ”€â”€ rag_service.py       # Main RAG orchestrator
â”‚   â”‚   â””â”€â”€ llm_service.py        # LLM integration
â”‚   â”‚
â”‚   â”œâ”€â”€ interfaces/               # Service abstractions
â”‚   â”‚   â””â”€â”€ llm_service.py       # LLM service interface
â”‚   â”‚
â”‚   â”œâ”€â”€ routers/                 # API routes
â”‚   â”‚   â”œâ”€â”€ index.py            # Basic endpoints (/ask, /chat, /stream)
â”‚   â”‚   â””â”€â”€ rag.py              # RAG endpoints
â”‚   â”‚
â”‚   â”œâ”€â”€ schemas/                 # Request/Response models
â”‚   â”‚   â”œâ”€â”€ base.py             # Unified response format
â”‚   â”‚   â”œâ”€â”€ requests.py         # Request schemas
â”‚   â”‚   â”œâ”€â”€ responses.py        # Response schemas
â”‚   â”‚   â”œâ”€â”€ rag_requests.py     # RAG request schemas
â”‚   â”‚   â””â”€â”€ rag_responses.py    # RAG response schemas
â”‚   â”‚
â”‚   â”œâ”€â”€ middleware/              # Middleware
â”‚   â”‚   â””â”€â”€ error_handler.py    # Error handling
â”‚   â”‚
â”‚   â””â”€â”€ utils/                   # Utilities
â”‚       â”œâ”€â”€ logger.py            # Logging configuration
â”‚       â”œâ”€â”€ exceptions.py        # Custom exceptions
â”‚       â”œâ”€â”€ response_helpers.py  # Response helpers
â”‚       â””â”€â”€ prompts.py          # Prompt templates
â”‚
â”œâ”€â”€ data/                        # Vector database storage
â”‚   â”œâ”€â”€ faiss_index             # FAISS index file
â”‚   â””â”€â”€ faiss_index.metadata.pkl # Metadata file
â”‚
â”œâ”€â”€ .env                         # Environment variables
â”œâ”€â”€ requirement.txt              # Python dependencies
â””â”€â”€ README.md                    # This file
```

---

## ğŸ“¡ API Endpoints

### Basic LLM Endpoints

#### `POST /ask` - Simple LLM Response
```bash
curl -X POST "http://localhost:8000/ask" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "What is machine learning?"}'
```

#### `POST /chat` - Chat with System Prompt
```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "system_prompt": "You are a helpful assistant.",
    "user_prompt": "Explain quantum computing"
  }'
```

#### `POST /stream` - Streaming Response
```bash
curl -X POST "http://localhost:8000/stream" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Tell me a story"}' \
  --no-buffer
```

### RAG Endpoints

#### `POST /rag/ingest` - Ingest Document
Process and store a PDF document in the vector database.

```bash
curl -X POST "http://localhost:8000/rag/ingest" \
  -H "Content-Type: application/json" \
  -d '{"pdf_path": "document.pdf"}'
```

**Response:**
```json
{
  "message": "Document ingested successfully",
  "status_code": 200,
  "data": {
    "status": "success",
    "chunks_created": 15,
    "embeddings_generated": 15,
    "total_vectors": 15,
    "source": "document.pdf"
  }
}
```

#### `POST /rag/query` - Query with RAG
Query the system using RAG (Retrieval-Augmented Generation).

```bash
curl -X POST "http://localhost:8000/rag/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is the main topic?",
    "top_k": 3
  }'
```

**Response:**
```json
{
  "message": "Query processed successfully",
  "status_code": 200,
  "data": {
    "answer": "The main topic is...",
    "context": ["chunk1", "chunk2", "chunk3"],
    "sources": [
      {
        "source": "document.pdf",
        "chunk_id": 5,
        "similarity_score": 0.89
      }
    ],
    "query": "What is the main topic?"
  }
}
```

#### `POST /rag/compare` - Compare RAG vs Non-RAG
Compare answers with and without RAG.

```bash
curl -X POST "http://localhost:8000/rag/compare" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is the main topic?",
    "top_k": 3
  }'
```

**Response:**
```json
{
  "message": "Comparison completed successfully",
  "status_code": 200,
  "data": {
    "query": "What is the main topic?",
    "rag_answer": "Based on the documents...",
    "non_rag_answer": "I don't have specific information...",
    "rag_sources": [...],
    "rag_context_count": 3,
    "comparison": {
      "rag_has_sources": true,
      "rag_answer_length": 450,
      "non_rag_answer_length": 120
    }
  }
}
```

#### `GET /rag/stats` - Get Statistics
Get statistics about the vector store.

```bash
curl -X GET "http://localhost:8000/rag/stats"
```

---

## ğŸ“ Understanding the Components

### 1. Document Processor
- **Purpose**: Extract text from PDFs and split into chunks
- **Key Features**: Sentence boundary detection, overlap handling
- **File**: `app/services/document_processor.py`

### 2. Embedding Service
- **Purpose**: Convert text to numerical vectors
- **Model**: OpenAI `text-embedding-3-small` (1536 dimensions)
- **File**: `app/services/embedding_service.py`

### 3. Vector Store (FAISS)
- **Purpose**: Store and search vectors efficiently
- **Type**: Local FAISS index (no cloud needed)
- **File**: `app/services/vector_store.py`

### 4. RAG Service
- **Purpose**: Orchestrate the complete RAG pipeline
- **Features**: Query enhancement, context filtering, answer generation
- **File**: `app/services/rag_service.py`

---

## ğŸ”§ Configuration

### Environment Variables

```env
# Required
OPENAI_API_KEY=your_api_key_here

# Optional (with defaults)
OPENAI_MODEL=gpt-3.5-turbo
REQUEST_TIMEOUT=30
STREAM_TIMEOUT=60
MAX_MESSAGE_LENGTH=10000
MAX_SYSTEM_MESSAGE_LENGTH=5000
```

### RAG Configuration

Default settings in `rag_service.py`:
- **Chunk Size**: 1000 characters
- **Chunk Overlap**: 200 characters
- **Embedding Model**: `text-embedding-3-small`
- **Vector Dimension**: 1536
- **Top-K Retrieval**: 3-5 chunks
- **Similarity Threshold**: 0.3

---

## ğŸ“– Step-by-Step Tutorial

### Step 1: Prepare Your Document

Place your PDF file in the project root:
```bash
cp your_document.pdf .
```

### Step 2: Ingest the Document

```bash
curl -X POST "http://localhost:8000/rag/ingest" \
  -H "Content-Type: application/json" \
  -d '{"pdf_path": "your_document.pdf"}'
```

**What happens:**
1. PDF is loaded and text extracted
2. Text is split into chunks
3. Each chunk is converted to an embedding (1536 numbers)
4. Embeddings are stored in FAISS vector database
5. Index is saved to `data/faiss_index`

### Step 3: Query Your Documents

```bash
curl -X POST "http://localhost:8000/rag/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are the key points?",
    "top_k": 3
  }'
```

**What happens:**
1. Query is enhanced (entity resolution)
2. Query is converted to embedding
3. FAISS searches for similar chunks
4. Top-K chunks are filtered and ranked
5. Context is injected into prompt
6. LLM generates answer using context

### Step 4: Compare RAG vs Non-RAG

```bash
curl -X POST "http://localhost:8000/rag/compare" \
  -H "Content-Type: application/json" \
  -d '{"query": "What are the key points?"}'
```

See the difference between:
- **RAG**: Answers with document context
- **Non-RAG**: Answers without context

---

## ğŸ¯ Key Concepts Explained

### What are Embeddings?

Embeddings are numerical representations of text meaning:
- Similar texts â†’ Similar vectors
- Enables semantic search (meaning-based, not keyword-based)
- Example: "Backend Engineer" and "Software Developer" have similar vectors

### What is FAISS?

FAISS (Facebook AI Similarity Search):
- **Local library** (not a cloud service)
- Stores vectors efficiently
- Performs fast similarity search
- No credentials or cloud setup needed

### What is Vector Dimension?

- **Dimension** = Number of numbers in each vector
- **1536** = OpenAI's embedding model creates 1536 numbers per text
- More dimensions = More detailed representation

---

## ğŸ” Example Workflow

### Complete Example

```bash
# 1. Start server
python main.py

# 2. Ingest document
curl -X POST "http://localhost:8000/rag/ingest" \
  -H "Content-Type: application/json" \
  -d '{"pdf_path": "document.pdf"}'

# 3. Query with RAG
curl -X POST "http://localhost:8000/rag/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What technologies are mentioned?",
    "top_k": 3
  }'

# 4. Compare results
curl -X POST "http://localhost:8000/rag/compare" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What technologies are mentioned?",
    "top_k": 3
  }'
```

---

## ğŸ§ª Testing

### Test RAG System

1. **Ingest a document**
2. **Query with various questions**
3. **Compare RAG vs non-RAG**
4. **Check statistics**

### Expected Results

**With RAG:**
- Specific answers from your documents
- Accurate information
- Source citations
- Context-aware responses

**Without RAG:**
- Generic answers
- May say "I don't have information"
- No citations

---

## ğŸš§ Future Enhancements

### Planned Features
- ğŸ”„ **MCP Integration**: Model Context Protocol support
- ğŸ“Š **Advanced Analytics**: Query performance metrics
- ğŸ” **Hybrid Search**: Combine keyword + semantic search
- ğŸ“ˆ **Re-ranking**: LLM-based result re-ranking
- ğŸŒ **Multi-document Support**: Process multiple PDFs
- ğŸ” **Access Control**: Document-level permissions

---

## ğŸ“š Learning Resources

### Understanding RAG
- [RAG Flow Explanation](RAG_FLOW_EXPLANATION.md) - Complete RAG flow
- [Vector Database Explained](VECTOR_DATABASE_EXPLAINED.md) - How vector DBs work
- [FAISS Explanation](FAISS_EXPLANATION.md) - FAISS details

### Code Documentation
- [RAG Improvements](RAG_IMPROVEMENTS.md) - Optimization techniques
- [RAG Usage Guide](RAG_USAGE_GUIDE.md) - Usage examples
- [Complete Status](RAG_COMPLETE_STATUS.md) - Implementation status

---

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:
- Additional document formats (DOCX, TXT, etc.)
- More embedding models
- Advanced chunking strategies
- Performance optimizations
- MCP integration examples

---

## ğŸ“ License

MIT License - Feel free to use this project for learning and development.

---

## ğŸ™ Acknowledgments

- OpenAI for embedding and LLM APIs
- Facebook AI Research for FAISS
- FastAPI for the web framework
- PyPDF2 for PDF processing

---

## ğŸ“ Support

For issues, questions, or contributions:
- Open an issue on GitHub
- Check the documentation files
- Review the code comments

---

## ğŸ“ Educational Purpose

This repository is designed to:
- âœ… Teach GenAI fundamentals
- âœ… Demonstrate RAG implementation
- âœ… Show vector database usage
- âœ… Provide production-ready code
- âœ… Serve as a learning resource

**Perfect for:**
- Learning RAG from scratch
- Understanding vector databases
- Building production RAG systems
- Experimenting with GenAI

---

**Happy Learning! ğŸš€**
